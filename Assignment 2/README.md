## Table of Contents

1. [Theory Problems](#theory-problems)
   - Regularization
   - MLP (Multi-Layer Perceptron)
   - Optimization

2. [Implementation](#implementation)
   - [Multi-Layer Neural Network](#multi-layer-neural-network)
   - [Forward Forward Algorithm](#forward-forward-algorithm)
  
## Theory Problems

### Theoretical Problems

In the Theoretical problems section, you'll find explanations and solutions for regularization, Multi-Layer Perceptrons (MLP), and optimization in deep learning. The regularization part includes practical solutions and explanations. The MLP section covers the theory behind Multi-Layer Perceptrons with straightforward answers and mathematical explanations. Additionally, the optimization part discusses theoretical challenges related to optimizing deep learning models, offering insights into effective training techniques.

## Implementation

### Multi-Layer Neural Network

I implemented a Multi-Layer Neural Network from scratch, incorporating the Adam and SGD optimization algorithms, along with dropout for enhanced performance. The implementation is structured and well-documented to facilitate understanding.

- `fully_connected_networks.py`: Python script containing the neural network implementation.
- `fully_connected_networks.ipynb`: Jupyter notebook demonstrating the usage of the neural network implementation.

### Forward Forward Algorithm

I implemented the forward forward algorithm based on the paper by Geoffrey Hinton. The implementation was tested on the MNIST dataset, showcasing its effectiveness in both supervised and unsupervised learning scenarios.

- **File Structure:**
  - `Forward-Forward Network.ipynb`: Jupyter notebook containing the implementation code for the forward propagation algorithm.

## Acknowledgments

- Geoffrey Hinton for the insightful paper that inspired the forward forward algorithm.
