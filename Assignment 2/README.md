# Deep Learning Assignment 2 - Neural Networks, Optimization, Regression

## Overview

This assignment covers building and training neural networks, optimization algorithms, and regression. 

Topics include:

- Fully-connected neural networks
- Backpropagation  
- Optimization algorithms like SGD, Momentum, RMSProp, Adam
- Regularization techniques like dropout   
- Linear regression
- Implementing a novel "forward-forward propagation" algorithm

## Theory Questions and Code Tasks

### Theory Questions

The assignment includes theoretical questions on:

- Neural network convergence
- Ensemble methods like model committees 
- Computing gradients and gradient descent optimization
- Adaptive optimization algorithms like Adam
- Dropout as a regularization technique

### Coding Tasks  

The coding portion involves:

- Fully connected neural network implementation
- Forward-forward propagation algorithm
- Training models on MNIST dataset
- Analyzing accuracy and convergence

## Fully Connected Neural Network Implementation

- Implemented building blocks of neural networks like layers, loss functions, optimizers
- Built a fully-connected network with configurable depth   
- Trained the network on MNIST dataset
- Tested different regularization techniques like dropout

## Forward-Forward Propagation

- Implemented the forward-forward propagation algorithm described in [this NeurIPS paper](https://arxiv.org/abs/2111.05368)  
- Generated positive and negative samples to train the network without backpropagation
- Tested model in supervised and unsupervised settings on MNIST
- Added a linear classifier on top for inference

## Contact 

Please reach out with any questions!

Email: m.j.amin200@gmail.com
