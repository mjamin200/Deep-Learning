# Deep Learning Assignment 4 - RNN & VAE

## Overview

This assignment covers RNN, BiLSTM, LSTM, autoencoder, and sequence-to-sequence models.

## Topics

- Analyzing molecular strings with LSTM, BiLSTM, FC
- Implementing convolutional variational autoencoder 
- Fine-tuning GPT-2 model for poetry generation
- Building vector quantized variational autoencoders

## Tasks

### Theory Questions

- Advantages of LSTM for sequence modeling
- Latent space in variational autoencoders
- Vector quantization for discrete latent representations

### Coding Tasks

- Evaluated LSTM, BiLSTM, FC on molecular strings 
- Implemented convolutional and vanilla VAE on MNIST
- Fine-tuned Persian GPT-2 model on Ferdowsi's poems
- Built VQ-VAE model and tested on MNIST and color MNIST

## Acknowledgements

Thanks to Aaron van den Oord et al. for the VQ-VAE paper:
[Neural Discrete Representation Learning](https://arxiv.org/pdf/1711.00937.pdf)

## Contact

Please reach out with any questions!

Email: m.j.amin200@gmail.com
